# Section 2 — Synthetic Event Generation for Testing

---

## 1. Context in LLMOps

In LLMOps, the hardest issues to debug often happen **infrequently** and under specific sequences of actions.
You cannot rely solely on *real* production runs to validate your monitoring pipeline — you need **synthetic event generation** to:

* Stress-test ingestion and visualization layers.
* Simulate rare failures, retries, and loops.
* Verify metrics calculations and event sequence analysis work as intended.
* Train operational staff on interpreting telemetry without affecting production workloads.

In this section, we simulate **hundreds to thousands of agent runs** with diverse outcomes so we can later:

* Plot error rates over time.
* Spot loops and retries in timelines.
* Test dashboard performance under load.

---

## 2. Event Types in This Section

We expand the event set from Section 1:

* **`run_start`** — start of a synthetic run.
* **`action_start`** — start of an action/tool call.
* **`action_result`** — result of the action (with `status="ok"` or `"error"`).
* **`run_ok`** — successful completion of a run.
* **`run_fail`** — run terminated due to failure.
* *(optional but useful in synthetic data)*:

  * `metric` — emit synthetic latency or token count.
  * `span_start` / `span_end` — if you want hierarchical tracing.

Even if your production system doesn’t emit all of these yet, synthetic data should **exercise the full schema** you might need.

---

## 3. How the Code Works

**Core control flow:**

1. **`simulate_run()`**

   * Creates a new `run_id`.
   * Emits `run_start`.
   * Randomly selects how many actions (steps) the run will have.
   * For each action:

     * Emits `action_start` with action name and step number.
     * Waits a short random time (simulating compute or network delay).
     * With configurable probability (`error_rate`), emits an error outcome.

       * On error, emits `run_fail` and stops further actions.
     * Otherwise, emits `action_result` with simulated metrics (`latency_ms`, `tokens`).
   * If all actions succeed, emits `run_ok`.

2. **Batch generation**

   * `simulate_many_runs()` loops for `n_runs` and writes all events to file.
   * You can control randomness via `random.seed()` for reproducibility.

3. **Performance knobs**

   * `error_rate` — probability that an action fails.
   * `n_actions_range` — min/max number of actions per run.
   * `latency_range` — simulated latency distribution.
   * `tokens_range` — simulated token count.

---

## 4. Strategies for Synthetic Event Generation

**Basic strategies:**

* **Uniform randomness**
  Easiest to implement, ensures coverage but not realistic patterns.

* **Weighted scenarios**
  Create distinct scenarios (e.g., “happy path”, “retry storm”, “loop”) and assign probabilities.

* **Temporal correlations**
  Make failures more likely after certain actions, or make later actions slower.

* **Sequence injections**
  Force certain sequences (e.g., action1 → error → retry → fail) to test timeline analysis.

**Advanced strategies:**

* **Property-based generation**
  Use libraries like Hypothesis to systematically cover edge cases.

* **Replay real runs**
  Anonymize and replay production events with modifications.

* **Failure bursts**
  Introduce periods of high error rate to test incident detection logic.

---

## 5. What to Look Out For

* **Schema coverage**: Synthetic runs should produce every `kind` of event your downstream code needs to handle.
* **Value range coverage**: Ensure metrics (latencies, token counts) cover realistic min/max ranges.
* **Temporal realism**: Even synthetic delays should be in realistic ranges so duration calculations are meaningful.
* **Volume handling**: Verify that ingestion and processing can handle the number of events you generate.

---

## 6. Insights for Real-world Projects

* Synthetic data is not just for development — it can be run in **staging pipelines** continuously to detect regressions in monitoring code.
* You can combine synthetic events with **chaos engineering** principles: intentionally break a component and see if events + metrics reveal the problem.
* Over time, your synthetic generator can evolve to model new agent behaviors and integrate with load testing.

---

## 7. Gotchas and Pitfalls

* **Overfitting to synthetic patterns**: Dashboards tuned to synthetic data may fail to highlight real-world anomalies — mix real and synthetic data in testing.
* **Unrealistic distributions**: If synthetic error rates or latencies are unrealistic, downstream alert thresholds will be wrong.
* **Clock skew**: If you simulate delays incorrectly, timestamps may appear out of order — downstream processing might reject these events.

---

## 8. Example Real-world Testing Scenarios

**Scenario 1 — High-latency tool**

* Set `latency_range=(1.0, 5.0)` seconds for one specific action.
* Verify dashboard shows this in timelines.

**Scenario 2 — Retry loops**

* Force one action to fail 3 times before succeeding.
* Check if sequence analysis flags this as a loop.

**Scenario 3 — Cascade failures**

* Introduce a failure in the first action that halts all subsequent steps.
* See if this shows up in aggregated failure causes.

