# Section 7 — Instrumentation Patterns and Hooks

---

## 1. Purpose and scope

* Instrumentation is the practice of inserting **observation points** into agent code so that important events are emitted automatically.
* The goal is to collect **consistent, structured data** without requiring ad-hoc logging or manual event creation for each new feature.
* This section focuses on **patterns** for synchronous, asynchronous, and per-token instrumentation that work on a single laptop and can be extended to production.

---

## 2. Types of events instrumentation can emit

1. **Lifecycle events**

   * `run_start`, `run_ok`, `run_fail` — emitted at workflow start and end.
2. **Span events**

   * `span_start`, `span_end` — emitted when entering and exiting a scoped operation (e.g., tool call, function).
3. **Action events**

   * `action_start`, `action_result` — for discrete steps in a run.
4. **Retry events**

   * `retry` — on repeated attempts of the same action.
5. **Metric hooks**

   * `metric` — direct emission of values (latency, token counts, GPU usage).
6. **Token-level events**

   * `token_start`, `token` — per-token emission from LLM streaming API.

---

## 3. Synchronous instrumentation pattern

**Decorator-based spans**:

```
def instrumented_action(name):
    def decorator(fn):
        def wrapper(*args, **kwargs):
            span_id = uuid4()
            emit("span_start", name=name, span_id=span_id)
            t0 = perf_counter()
            try:
                result = fn(*args, **kwargs)
                duration = perf_counter() - t0
                emit("span_end", name=name, span_id=span_id, status="ok", duration_s=duration)
                return result
            except Exception as e:
                duration = perf_counter() - t0
                emit("span_end", name=name, span_id=span_id, status="error", error=str(e), duration_s=duration)
                raise
        return wrapper
    return decorator
```

* Pros: easy to apply, clear lifecycle.
* Cons: synchronous only; no async support.

---

## 4. Asynchronous instrumentation pattern

**Async context manager spans**:

```
class instrument_span:
    def __init__(self, name):
        self.name = name
        self.span_id = uuid4()
    async def __aenter__(self):
        emit("span_start", name=self.name, span_id=self.span_id)
        self.t0 = perf_counter()
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        duration = perf_counter() - self.t0
        status = "ok" if exc_type is None else "error"
        emit("span_end", name=self.name, span_id=self.span_id, status=status, error=str(exc_val) if exc_val else None, duration_s=duration)
```

* Pros: works with async agent/tool calls.
* Cons: must be used explicitly in `async with` blocks.

---

## 5. Per-token timing hooks

* For streaming LLM APIs, instrumentation can wrap the token callback to emit events for:

  * First token time (TTFT — time to first token)
  * Tokens per second throughput
* Implementation:

```
def token_emitter():
    first_token_time = None
    def on_token(token):
        nonlocal first_token_time
        now = perf_counter()
        if first_token_time is None:
            first_token_time = now
            emit("metric", name="time_to_first_token", value=now - start_time)
        emit("metric", name="token_emitted", value=1)
    return on_token
```

* This enables analysis of streaming latency patterns and throughput bottlenecks.

---

## 6. GPU usage checks

* On a single laptop with GPU, check availability at run start:

```
import torch
if torch.cuda.is_available():
    emit("metric", name="gpu_memory_free_mb", value=torch.cuda.mem_get_info()[0] / 1024**2)
else:
    emit("metric", name="gpu_available", value=0)
```

* Can be emitted periodically for long runs.

---

## 7. Testing instrumentation locally

* **Synthetic functions**: decorate with `instrumented_action` and induce random delays/errors.
* **Async tool mocks**: use `instrument_span` with simulated I/O delays.
* **Token simulation**: feed token emitter with fake token stream at varied speeds.

---

## 8. What to look for in production

* Missing `span_end` events → indicates exceptions escaping without proper handling.
* Multiple `span_start` without matching `span_end` → possible leaks in async contexts.
* TTFT spikes → model cold starts, API throttling.
* Token throughput drops → network bottlenecks or degraded model performance.

---

## 9. Common pitfalls and mitigations

* **Async callback mismatch**: ensure async wrappers are truly async, otherwise you hit `takes 0 positional arguments but 1 was given` errors.
* **Blocking in async spans**: avoid sync calls in async context managers; they block event loops.
* **Heavy emitters**: ensure emission code is lightweight and non-blocking; buffer to disk if needed.
* **Double emission**: guard against decorators wrapping the same function twice.

---

## 10. Takeaways

* Standardize span and action instrumentation early; retrofitting later is costly.
* Keep emitters lightweight and safe in both sync/async contexts.
* Token-level instrumentation is valuable for LLM latency analysis, even locally.
* Always test with synthetic delays and errors to confirm the instrumentation flow.