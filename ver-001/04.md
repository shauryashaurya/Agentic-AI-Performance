# Section 4 — Metrics Collection & Visualization (Rates, Latencies, Errors, Retries)

## 1. Purpose and scope

* Turn raw events into **quantitative signals** that answer: how often, how fast, how bad, where.
* Produce **stable, typed time series** suitable for alerting and trend analysis.
* Maintain **traceability** back to runs/sequences for root cause.

What we measure here:

* **Throughput** (runs started per time unit)
* **Outcome rates** (success/failure)
* **Latency distributions** (mean/median/tails)
* **Retries** (attempt count distributions)
* **Error rates** (by action/tool)

---

## 2. Event → metric mapping

* **Throughput**: count `run_start` per minute (`minute` column).
* **Success/failure**: union of `run_ok` and `run_fail` by `run_id` (deduplicate).
* **Latency**: use `duration_s` from `span_end` grouped by `name`.

  * Prefer emitted `duration_s` to recomputing from start/end timestamps (avoids clock skew).
* **Retries**: use `action_result` with `action` and `attempts` (coerce to int).
* **Error rate by action**: share of `action_result.ok == False` within each `action`.

Notes:

* Keep metric tables **narrow and typed**. Do not carry Python objects (`attrs`, `data`) into metric frames.
* Always guard for **empty slices** and ensure **numeric dtypes** before plotting.

---

## 3. Windowing and granularity

* **Minute** is a good default window locally; switch to **5m/15m** bins for long sessions.
* Use **flooring** (`ts.dt.floor('min')`) to avoid off-by-one bucket issues.
* For tail latency SLOs, compute percentiles per **span type** (e.g., `tool:search`, `act:generate`).

---

## 4. Core computations (recommended formulas)

* **Runs per minute (RPM)**
  `rpm = df[df.is_run_start].groupby('minute').size()`

* **Success rate**

  ```
  runs = concat(run_ok, run_fail).drop_duplicates('run_id')
  success_rate = runs.ok.mean()
  ```

* **Latency summary per span name**

  ```
  g = spans.groupby('name')['duration_s']
  count, mean, median, max
  p90 = g.quantile(0.90); p95; p99
  ```

* **Attempts distribution**

  ```
  ar = action_result[['action','attempts']]
  counts = ar.groupby(['action','attempts']).size().unstack(fill_value=0)
  ```

* **Error rate by action**

  ```
  err = (~action_result.ok.fillna(False)).groupby(action).mean()
  ```

---

## 5. Visualization guidance (matplotlib-only constraints)

* Bar charts are sufficient for these local diagnostics:

  * **RPM**: bar by minute (keep ≤60 bars per figure for readability).
  * **Latency**: for each `name`, bar for `mean`, `median`, `p90`, `p95`, `p99`.
  * **Retries**: stacked bar by `attempts` columns.
  * **Error rates**: bar by `action`.

Defensive practices:

* Before plotting, select numeric dtypes (`select_dtypes(include=[np.number])`).
* Bail out with a clear message if the frame/series is empty after coercion.
* Keep figure generation side-effect-free (each plot writes to a file and closes the figure).

---

## 6. Error taxonomy and enrichment (optional, valuable)

* Derive **error class** from `action_result` and terminal events:

  * From `error` string on `run_fail` or `span_end.status=="error"`.
  * Map to categories: `timeout`, `validation_error`, `tool_unavailable`, `rate_limit`, `unknown`.
* Compute **error rate by category** per action and over time.
* Track **first-failure action** per failed run to locate chokepoints.

---

## 7. Interpreting results (what to look for)

* **RPM spikes**: sudden changes often correlate with failures or queuing. Check error rate and latency in the same window.
* **High p95/p99**: tail growth without mean movement indicates heavy contention, cold starts, or retries.
* **Retry stacks >2**: persistent retries suggest misconfigured thresholds or flaky tools; see Section 5 for loop patterns.
* **Asymmetric error rates**: one action or tool dominates failures; isolate by span name and payload class (if safe).
* **Zero variance**: suspiciously perfect metrics usually mean missing data or type coercion failure.

---

## 8. SLO/SLA framing (production-ready thinking)

* Choose **user-facing SLOs** on spans that map to noticeable latency:

  * e.g., `act:generate p95 < 1.5s`, `tool:search p95 < 300ms`.
* Track **availability** as `(run_ok / (run_ok + run_fail))`.
* Alert on **error budget burn**: sliding windows (e.g., 1h, 6h). Start with static thresholds locally; add anomaly detection later.

---

## 9. Robustness and correctness checks

* **Deduplication**: ensure each `run_id` contributes one outcome.
* **Type safety**: `attempts`, `duration_s` must be numeric; coerce with `errors='coerce'` and drop nulls for aggregates.
* **Sampling bias**: if you sample events, document the rate and scale metrics appropriately.
* **Clock sanity**: enforce UTC; reject negative `duration_s` or flag them for inspection.

---

## 10. Testing strategy for metrics layer

* **Golden windows**: canned event sets with expected RPM, success rate, and percentile tables.
* **Perturbation tests**: introduce NaNs, missing `attempts`, or mixed types to verify guards prevent exceptions.
* **Volume tests**: thousands of spans to make sure percentile computation stays performant on a laptop.

---

## 11. Production takeaways

* Separate **metric computation** from **event ingestion**; persist both.
* Make each metric function **pure and idempotent**; it should accept a DataFrame slice and return a typed Series/DataFrame.
* Prefer **pXX percentiles** over means for latency health; watch `p95` and `p99`.
* Keep **plots optional**; the source CSVs/tables are the real artifacts used by CI or dashboards.

---

## 12. Common pitfalls and fixes

* **“no numeric data to plot”**: coerce to numeric and check empties before plotting.
* **Arrow/Parquet type errors**: never write raw dicts/lists; JSON-encode into `*_json` columns first.
* **Double-counted runs**: always deduplicate outcomes by `run_id`.
* **Silent data loss**: always flush writers; in batch jobs, assert non-empty outputs and print row counts.

