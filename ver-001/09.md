# Section 9 — Local LLM Integration with Ollama and Per-Token Timing
  
Install Ollama for this part.  
prereqs  
    install `ollama` and run the server in the background  
    pull a small model once, for example: `ollama pull phi3:mini` or `ollama pull qwen2.5:0.5b` or `ollama pull llama3.2:1b`  
    default config here uses `phi3:mini`, change `MODEL_NAME` in code if you prefer another tiny model  
  
ensure sections 1–8 files exist if you want to reuse the same data folder  
run this sibling script directly   
    `python 09_ollama_local_llm.py`     

then fold events into the pipeline   
    `python 03_event_schema_and_logging.py`   
    `python 04_metrics_basics.py`     
    `python 05_sequence_analysis.py`     
    `python 06_combined_dashboard.py`   
       
---

## 1. Purpose and scope

* Run an LLM **entirely on a laptop** (CPU or small GPU) via **Ollama**.
* Stream tokens from the local server and **instrument**:

  * time to first token (TTFT)
  * inter-token latency
  * throughput (tokens/sec)
* Emit events compatible with earlier sections so the same ingestion, metrics, and sequence analysis apply.

---

## 2. Architecture and data flow

1. Client script opens an HTTP connection to the local Ollama server (`/api/generate`, `stream=true`).
2. Server returns a **newline-delimited stream** of JSON chunks.
3. Client:

   * timestamps the start of the request
   * for each streamed chunk:

     * appends the partial text
     * records timing deltas to compute **first token** and **per-token** latencies
     * emits `metric` events for token timings
   * on completion:

     * emits `action_result` with the token count
     * emits `run_ok` (or `run_fail` on errors)

---

## 3. Event types produced

* `run_start` — marks the beginning of a generation request.
* `span_start` / `span_end` — wrap the overall `act:generate` span.
* `metric` — per-token latencies, first token latency, tokens/sec.
* `action_result` — `action="generate"`, `ok`, `attempts`, and minimal data (e.g., token count).
* `run_ok` / `run_fail` — terminal outcome.
* `ollama_tags` (optional) — server health snapshot at start (list of installed models).

These align with the schema defined in Section 3 (typed columns, JSON-encoded `attrs_json`/`data_json` when persisted).

---

## 4. How the streaming loop is instrumented

1. Record `t_start = perf_counter()` just before sending the HTTP request.
2. For each streamed JSON line:

   * On the **first** content chunk, compute `first_token_latency_s = now - t_start`, emit `metric`.
   * For **subsequent** chunks, compute `inter_token_latency_s = now - last_chunk_time`, emit `metric` with `attrs.token_index`.
3. After the stream completes:

   * Compute total tokens and `throughput = tokens / total_duration`.
   * Emit `metric` for throughput and `action_result` with token count.

Notes:

* Treat each chunk as a “token” for timing. This is approximate but sufficient for **relative** performance and loop detection.
* If your server exposes actual token counts, add them to `data`.

---

## 5. Configuration recommendations

* **Model selection**: start with small models (e.g., `phi3:mini`, `qwen2.5:0.5b`, `llama3.2:1b`) to keep memory below \~4 GB.
* **Temperature**: begin with low values (e.g., `0.1–0.3`) to reduce output variance during tests.
* **Timeouts**: set a conservative request timeout (e.g., 30–60 s) and emit `run_fail` with reason on timeout.
* **Prompt discipline**: keep a fixed system prompt for comparability across runs; log it in `attrs`.

---

## 6. GPU checks and resource context

* Before generation:

  * Attempt a **GPU availability** check (e.g., `torch.cuda.is_available()` if `torch` is installed).
  * Optionally query `nvidia-smi` for memory totals if PyTorch is not available.
* Emit a one-time `gpu_info` or `metric` events:

  * `gpu_available`
  * `gpu_mem_total_mb` / `gpu_mem_free_mb` (if available)
* This provides context for variability in TTFT and throughput.

---

## 7. Failure handling and retries

* Classify common failure cases and emit consistent `reason` values:

  * `ollama_port_closed`
  * `http_error_<status>`
  * `stream_parse_error`
  * `generate_error`
  * `timeout`
* For **retry policies**:

  * If the transport fails before any token arrives, retry up to N times with backoff; record `attempts` in `action_result`.
  * If the stream starts and then fails mid-way, treat it as a **partial** success; decide whether to retry based on business rules. Emit a distinct `reason` (e.g., `stream_incomplete`).

---

## 8. Joining with previous sections (analysis workflow)

1. Run the Ollama script to produce events into `data/raw_events/`.
2. Re-run Section 3 ingestion to produce updated `events.parquet`.
3. Section 4 metrics will include:

   * TTFT distribution (from `metric name=first_token_latency_s`)
   * Token throughput distributions
   * Latency of `act:generate` spans
4. Section 5 sequence analysis can now include:

   * `act:generate` inside broader run sequences
   * loops around generation + downstream tools (e.g., `generate → validate → generate`)

---

## 9. Testing strategies (local, deterministic as possible)

* **Server reachability**: assert TCP port open before starting; emit `run_fail` if closed.
* **Model presence**: query `/api/tags` to confirm the model exists; if not, log a warning and continue (Ollama may auto-pull).
* **Repeatability**:

  * Fix seed where the server allows it (many do not); otherwise normalize with a fixed system prompt and temperature.
  * Run **short prompts** to keep total duration small and avoid CPU/GPU thermal throttling.
* **Degradation tests**:

  * Increase temperature to see if variability affects TTFT or throughput.
  * Introduce network latency (e.g., loopback shaping if available) and verify metrics reflect the change.
  * Run back-to-back generations to test cold-start vs warm-cache behavior.

---

## 10. Operational insights you can extract

* **TTFT vs model size**: small local models should show relatively low TTFT; large models increase TTFT even at similar throughput.
* **Throughput stability**: consistent tokens/sec suggests the server is not thermally throttling; drops over time may indicate thermal or memory constraints.
* **Effect of GPU memory**: when free GPU memory dips, expect TTFT increases or generation failures; correlate with `gpu_mem_free_mb`.
* **Retry value**: network-level retries may help only for early failures; mid-stream retries often produce duplicate or inconsistent output and should be avoided or de-duplicated downstream.

---

## 11. Integration guidance for productionizing locally-served LLMs

* **Emitter decoupling**: keep the HTTP client and the emitter separate; instrument via spans so you can swap emitters (file → API) without touching client logic.
* **Backpressure**: if you emit per-token metrics at high rates, buffer locally and flush periodically to avoid I/O becoming the bottleneck.
* **Redaction**: redact prompt/user text at the emitter if logs can be shared; store hashes or summaries instead of raw text where required.
* **Sampling**: allow sampling of per-token metrics (e.g., sample 10% of generations) to keep data volume manageable on laptops.

---

## 12. Common pitfalls and mitigations

* **Chunk ≠ token**: streamed chunks may not map 1:1 to tokens. For precise counts, rely on server metadata if available; otherwise use chunk-based timings as an approximation for **relative** analysis.
* **Event storms**: per-token events can be numerous; sample or aggregate (e.g., emit only inter-quartile stats every N tokens).
* **Blocking I/O**: ensure the HTTP read loop does not block emitter flushing; use small buffered writes.
* **Time calculations**: always use a **monotonic clock** (`perf_counter`) for latencies; never compute latencies from wall-clock timestamps.

---

## 13. Example mapping to our schema (concise)

* `run_start`: `task="ollama_generate"`, `attrs.model`, optional `attrs.temperature`.
* `span_start/end`: `name="act:generate"`, duration from `perf_counter`.
* `metric`: `name="first_token_latency_s"`, scalar; `name="token_latency_s"`, with `attrs.token_index`; `name="throughput_tokens_per_s"`.
* `action_result`: `action="generate"`, `ok`, `attempts`, `data.tokens`.
* `run_ok` / `run_fail`: terminal outcomes with `reason` on failure.

---

## 14. Possible Next steps

* If you need **exact** token counts, extend the parser to read server-provided token counters (if exposed) and include them in `action_result.data`.
* Add a small **comparison harness** that runs:

  * simulated tokens (Section 8)
  * local Ollama run (this section)
    and writes a side-by-side CSV of TTFT and throughput for regression checks.

---