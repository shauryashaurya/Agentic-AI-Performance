 # Section 6 — Combined Metrics + Event Pattern Analysis

---

## 1. Purpose and scope

* Metrics (Section 4) tell you **where** and **when** anomalies happen.
* Event timelines (Section 5) tell you **what exactly happened**.
* Combining them lets you:

  1. Link a spike in latency or errors to **specific action sequences**.
  2. Identify **root causes** faster by looking at patterns in failing runs.
  3. Quantify **impact** by counting how many runs exhibit the same failure mode.

The combined view is what makes LLMOps debugging practical at scale.

---

## 2. Key inputs for combined analysis

From **metrics layer**:

* per-minute run counts (`rpm`)
* success/failure rates
* latency stats per action/span
* error rates per action

From **timeline layer**:

* full ordered sequences of events per `run_id`
* loop/cycle flags
* first-failure action per run
* sequence hash or compressed representation

---

## 3. Linking metrics and patterns

We join the two data sets on `run_id`:

1. Filter runs of interest from metrics (e.g., all failed runs in a spike window).
2. Join with sequence table to see:

   * which actions preceded the failure
   * whether the run had loops or excessive retries
   * whether a specific tool/action correlates with higher latency or failure rate

Example workflow:

```
# runs that failed in the spike window
failed_runs = runs_df[
    (runs_df.ts >= spike_start) &
    (runs_df.ts <= spike_end) &
    (runs_df.status == "fail")
]

# join with sequence data
joined = failed_runs.merge(sequences_df, on="run_id", how="left")

# group by first_failure_action
pattern_stats = joined.groupby("first_failure_action").size()
```

---

## 4. Practical correlation cases

**Case 1 — Latency spike cause**

* Metric: p95 latency for `tool:search` jumped in a 5-minute window.
* Timeline correlation: those runs also show `tool:search` → `retry` → `tool:search` patterns.
* Likely cause: tool slow to respond, causing retries and tail growth.

**Case 2 — Failure cluster cause**

* Metric: sudden rise in `run_fail`.
* Timeline: majority of failed runs have `summarize → translate → summarize` loops.
* Likely cause: summarization output triggers a translation step that re-triggers summarization due to formatting error.

**Case 3 — Retry storm cause**

* Metric: retries per action jumped from avg 0.2 to 3.5 for `generate_report`.
* Timeline: most failing runs repeat `generate_report` → `save_draft` → `generate_report`.
* Likely cause: downstream service rejecting saved drafts, causing upstream to re-generate.

---

## 5. Representations for scalable analysis

1. **First failure action**

   * Extract the first `action_result` with `ok=False` per run.
   * Grouping by this field often reveals the main choke points.

2. **Sequence hash**

   * Hash the ordered `action` list (with/without parameters) to a fixed string.
   * Count occurrences; top hashes for failing runs often represent systemic bugs.

3. **Loop signature**

   * Represent loops as e.g., `A(3)` meaning action A repeated 3 times consecutively.
   * Easier to spot than reading raw sequences.

4. **Windowed aggregation**

   * Compute counts of each sequence hash in sliding windows to detect emerging failure patterns.

---

## 6. Visualization techniques

* **Stacked bar of failure causes** over time (x-axis = time, y-axis = run count, stack = first failure action).
* **Heatmap** of actions vs. failure rate (rows = actions, cols = windows, values = % failed).
* **Sequence prevalence chart**: top N sequence hashes by occurrence, colored by success/failure.

All of these can be generated with Matplotlib or Seaborn equivalents, but remember:

* Coerce counts to integers before plotting.
* Guard against empty DataFrames after filtering.

---

## 7. Testing strategy for combined analysis

1. **Synthetic injection**

   * Introduce known bad patterns into synthetic data; ensure combined analysis finds them.
2. **Window correctness**

   * Verify that time filtering (`spike_start`, `spike_end`) isolates expected runs.
3. **Hash stability**

   * Ensure identical sequences always hash to the same value across runs.
4. **Null handling**

   * Runs without sequences (e.g., ingestion gaps) should not crash the join.

---

## 8. Gotchas and mitigations

* **Parameter noise**: including raw parameters in sequence signatures may create too many unique hashes — consider parameter normalization.
* **Partial runs**: runs without terminal events can distort metrics; label them explicitly.
* **False correlations**: correlation does not imply causation — verify with targeted replay or controlled synthetic runs.
* **Overfitting to recent patterns**: failure patterns can change; keep historical baselines.

---

## 9. Takeaways for production

* Always join **run-level metrics** and **sequence patterns** for incident debugging.
* Maintain **a small set of stable features** for correlation (e.g., first\_failure\_action, sequence\_hash, retry\_count).
* Use these features to:

  * drive automated alerts (e.g., "loop pattern X seen in 20% of runs in last 10m")
  * prioritize engineering fixes (which action/tool is causing the most impact)
* Store both **raw sequences** and **derived signatures**; you’ll need both for analysis and explainability.
