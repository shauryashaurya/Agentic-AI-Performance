# Section 3 — Data Model for Metrics & Events (Ingestion, Normalization, Storage)

## 1. Why a data model matters for LLMOps

* Agent telemetry is multi-granular: run-level, span-level, action-level, token-level.
* You will compute both **time-series metrics** (rates, pXX latencies) and **structural analytics** (event sequences, loops).
* A consistent, typed model lets you:

  * join runs, spans, and actions reliably
  * compute robust metrics without ad-hoc parsing
  * avoid failures on schema drift or mixed Python objects

---

## 2. Event schema you should target (flat, append-only)

Use a **single, wide, append-only table** for raw ingestion. Keep it flexible but typed.

Required columns (always present):

* `ts: timestamp with timezone (UTC)`

  * use `datetime.now(timezone.utc)`
  * store as `TIMESTAMP_TZ` in columnar stores
* `event_id: string (uuid4)`
* `kind: string`

  * canonical values only (examples below)
* `run_id: string (uuid4)`

Optional columns (nullable, typed where possible):

* `span_id: string`, `parent_span_id: string`
* `name: string`
* `status: string`  # e.g., ok, error
* `error: string`   # short error info or repr
* `duration_s: float`
* `step: int`
* `action: string`
* `ok: boolean`
* `attempts: int`
* `reason: string`  # for terminal events like run\_fail
* `data_json: string`   # JSON-serialized payloads (see below)
* `attrs_json: string`  # JSON-serialized context (see below)

Derived columns (materialized at ingestion for convenience):

* `date: date`
* `minute: timestamp_tz truncated to minute`
* `hour: timestamp_tz truncated to hour`
* `is_span_start: boolean`
* `is_span_end: boolean`
* `is_run_start: boolean`
* `is_run_ok: boolean`
* `is_run_fail: boolean`
* `is_action_result: boolean`
* `is_retry: boolean`

Rationale:

* Put complex, heterogeneous Python objects (`attrs`, `data`) into **JSON strings** (`attrs_json`, `data_json`) to stay Arrow/Parquet-safe and avoid type errors. You can parse them on demand.
* Strongly type everything else so aggregations are safe and fast.

---

## 3. Canonical event kinds

Keep `kind` small and stable; avoid synonyms.

Minimal set for this workshop:

* lifecycle: `run_start`, `run_ok`, `run_fail`, `run_error`
* spans: `span_start`, `span_end`
* actions: `step_start`, `action_result`, `retry`
* analysis hooks (optional): `metric`, `counter`, `gpu_info`, `ollama_tags`, `loop_guard_triggered`

Notes:

* `span_start`/`span_end` are paired via `span_id`. Compute durations at emission or on read; if both exist, prefer the emitted `duration_s` (cheaper downstream).
* `action_result` should always carry `action`, `ok`, and `attempts`.

---

### 4) Normalization pipeline (ingest → clean → persist)

**Input**: one or many `.jsonl` files under `data/raw_events/`
**Output**: a typed Parquet at `data/derived/events.parquet` (+ small CSV sample)

Steps:

1. **Read JSONL**

   * tolerant reader, skip malformed lines, do not crash the batch
2. **Field presence**

   * ensure all columns from the schema exist; fill missing with `NULL`
3. **Type coercion**

   * `ts` → `datetime64[ns, UTC]`
   * numeric fields (`duration_s`, `step`, `attempts`) via `pd.to_numeric(..., errors="coerce")`
   * `ok` → pandas `boolean` (nullable)
4. **Object flattening**

   * convert `data` and `attrs` to strings:

     * if dict/list → `json.dumps`
     * if scalar → `str(x)`
     * if null → `NULL`
   * write as `data_json`, `attrs_json`
   * drop raw Python-object columns before Parquet write
5. **Derive convenience columns** (`date`, `minute`, flags)
6. **Sort** by `ts`, `run_id` for stable downstream ops (stable mergesort)
7. **Persist** as Parquet (columnar, compressed), plus a small CSV sample for eyeballing

Why Parquet:

* column pruning, vectorized IO, good Arrow compatibility
* works locally and scales to data lakes later with zero code changes

---

## 5. Computing first-level sanity metrics at ingestion

* **Event counts by `kind`**

  * helps spot schema drift or missing emitters
* **Run outcomes**

  * collect `run_ok` and `run_fail` by `run_id`, compute `success_rate`
* **Span latency overview**

  * `span_end` grouped by `name` → `count`, `mean`, `median`, `max`, `p90`, `p95`, `p99`
* **Attempts distribution**

  * from `action_result`: counts by `action` × `attempts`
* **Runs per minute**

  * `run_start` grouped by `minute` (good for later spike detection)

These checks are quick signals that ingestion is correct and the data is usable.

---

## 6. Strategies for handling schema evolution

* **Append-only, backward compatible**

  * new fields start `NULL` for older data; old code keeps running
* **Compatibility shims**

  * if a producer changes a field name, add a renaming step in ingestion (keep a mapping function)
* **Version stamping (optional)**

  * add `schema_version` in `attrs_json` when doing disruptive changes

---

## 7. Reliability and correctness concerns

* **Idempotency**

  * if you may re-run ingestion, de-duplicate by `(event_id)` or by `(ts, run_id, kind, span_id)` when `event_id` is missing
* **Ordering**

  * ingestion order is not guaranteed; always sort by `ts` then `run_id` when reconstructing sequences
* **Clock skew**

  * machines emitting events might not be time-synced; consider tolerating small negative durations and out-of-order spans
* **Late or missing span pairs**

  * if `span_start` is present without `span_end`, flag as open span; do not fail ingestion
* **Mixed types**

  * never write heterogeneous `object` columns to Parquet; always JSON-encode first

---

## 8. Privacy / security considerations

* **Redaction at emission**

  * redact keys at the emitter for request bodies, secrets, PII
* **Partitioning**

  * partition by `date` or `hour` to support retention policies
* **Access control**

  * if sharing event dumps, ensure sensitive fields are removed or redacted; prefer `attrs_json` with a vetted subset

---

## 9. Testing the ingestion layer

* **Golden files**

  * a small folder of known-good `.jsonl` samples with expected Parquet and summary stats
* **Schema assertions**

  * verify presence and dtypes of all columns after coercion
* **Edge-case fixtures**

  * missing fields, wrong types in `data`, large numeric strings, broken JSON lines
* **Volume test**

  * tens of MBs locally to validate memory use and write times

---

## 10. Performance and scalability on a single laptop

* **Batch reads/writes**

  * concatenate lists of dicts before DataFrame creation to minimize Python overhead
* **Column subset**

  * if you only need metrics, read only needed columns from Parquet
* **Compression**

  * Parquet defaults (snappy) are fine; avoid gzip JSONL for hot paths to keep it simple locally
* **Rotation for raw JSONL**

  * rotate by size (e.g., 20 MB) to keep editor/terminal usable

---

## 11. Example row shapes (after normalization)

`run_start`

```
ts=2025-08-10T12:00:03Z
event_id=uuid
kind=run_start
run_id=uuid
task="lookup thing_3"
data_json=NULL
attrs_json=NULL
...
```

`span_end` for a tool call

```
ts=2025-08-10T12:00:04Z
event_id=uuid
kind=span_end
run_id=uuid
span_id=uuid
parent_span_id=NULL
name="tool:search"
status="ok"
duration_s=0.184
error=NULL
attrs_json='{"q":"lookup thing_3 site:example.com"}'
```

`action_result` with retries

```
ts=2025-08-10T12:00:04Z
event_id=uuid
kind=action_result
run_id=uuid
action="search"
ok=false
attempts=2
data_json=NULL
attrs_json=NULL
```

`run_fail` with reason

```
ts=2025-08-10T12:00:05Z
event_id=uuid
kind=run_fail
run_id=uuid
reason="max_steps_exceeded"
```

---

## 12. What to look out for during analysis later

* **`span_end` without `span_start`**

  * check your emitters; this often points to exceptions not reaching `__exit__` hooks
* **`action_result` with `attempts` missing**

  * standardize the retry policy; missing attempts breaks retry metrics
* **`run_fail` without `reason`**

  * enforce reason strings; classification of incidents depends on it
* **`duration_s` zeros**

  * make sure durations are measured with a monotonic clock (`perf_counter`) and emitted on `span_end`

---

## 13. Takeaways for productionizing

* **Strongly type what you aggregate; JSON-encode everything else**
* **Emit correlation IDs everywhere** (`run_id`, `span_id`)
* **Derive convenience columns at ingestion** to simplify downstream code
* **Keep a tight, canonical set of event kinds** to avoid if/else sprawl
* **Design for partial data and late events**; do not fail pipelines on gaps

