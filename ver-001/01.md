# Section 1 — Agentic AI 101 & Basic Event Emission

## 1. Context in LLMOps

In **LLMOps** (Large Language Model Operations), AI agents are typically **long-lived, stateful, and multi-step**. Unlike stateless API calls, they:

* Maintain **context** across steps.
* Use **tools** or call APIs in sequence.
* May run for seconds to hours.
* Fail in **non-deterministic** ways due to stochasticity of the LLM and environment changes.
* Sometimes get stuck in loops or retry endlessly.

This makes **event-level visibility** critical. In traditional software, logs + metrics often suffice; in LLMOps, you also need **semantic and structural event logs** to debug complex, emergent failures.

Example: A customer support agent making repeated calls to the same API with slightly different inputs — traditional error metrics just say "high error rate", but events tell you **which sequence** led to that loop.

---

## 2. What is an Event?

An **event** is a structured, timestamped record describing something the system did or experienced.
Core properties for AI agent event logs:

* **Timestamp**: precise UTC-based time of occurrence.
* **Kind/type**: e.g., `run_start`, `action_result`.
* **Identifiers**:

  * `run_id` — groups all events from a single agent run.
  * `event_id` — unique to this event, used for tracing.
  * Optional `span_id` and `parent_span_id` — for hierarchical tracing.
* **Attributes (attrs)**: free-form key-value metadata relevant to the event (e.g., tool name, model parameters).
* **Status**: e.g., `ok`, `error`.
* **Duration**: time taken (for spans/actions).

Example:

```json
{
  "ts": "2025-08-10T13:21:04Z",
  "event_id": "3b9a5c40-52e1-4d21-9e38-f739ffde3aaf",
  "run_id": "5d48af8a-55b5-4ad5-b4de-97701d73f71f",
  "kind": "action_result",
  "action": "search_documents",
  "status": "ok",
  "duration_s": 0.238,
  "attrs": {"query": "GPU availability in cloud"},
  "data": {"num_results": 14}
}
```

---

## 3. Event Types in This Section

For the simplest monitoring, we use **run- and action-level** events:

* **`run_start`**: marks the beginning of a run.
* **`action_start`**: a single step/tool usage has started.
* **`action_result`**: result of that action (success, failure, retry, with details).
* **`run_ok`**: run completed successfully.
* (In later sections, we’ll add `run_fail`, `span_start`, `span_end`, `metric`, etc.)

This is sufficient to start understanding control flow and outcomes without overcomplicating the schema.

---

## 4. How the Code Works

**Core flow:**

1. **`now_iso()`**

   * Returns UTC timestamp in ISO 8601 format.
   * Uses `datetime.now(timezone.utc)` to avoid `utcnow()` deprecation.
   * `.replace('+00:00', 'Z')` is optional for the `Z` suffix.

2. **`eid()`**

   * Generates UUID4 strings for unique IDs.
   * Ensures no collisions even in high-volume event streams.

3. **`JsonlSink`**

   * Buffers event records in memory.
   * Flushes to disk when buffer reaches threshold or on demand.
   * Appends to `.jsonl` file (safe for incremental writes).

4. **`Emitter`**

   * Wraps `JsonlSink` to produce events with minimal boilerplate.
   * Handles base fields like `ts` and `event_id` automatically.

5. **Simulation**

   * Simulates an agent run with three `action_start` + `action_result` sequences.
   * Produces `run_start` at the beginning and `run_ok` at the end.

---

## 5. Strategies for Designing Event Schemas in LLMOps

* **Start minimal, evolve incrementally**
  Begin with run-level and action-level events. Add spans, metrics, and extra attrs later.

* **Separate `attrs` and `data`**

  * `attrs` for contextual metadata (e.g., model name, temperature, query).
  * `data` for structured outputs/results.

* **Include IDs for correlation**

  * `run_id` for grouping.
  * `span_id` + `parent_span_id` for hierarchies.
  * Optional `session_id` for multi-run user sessions.

* **Make `kind` consistent**

  * Avoid mixing synonyms (`run_failed`, `fail_run`, etc.).
  * Decide early and enforce.

* **Prefer UTC timestamps**

  * Always store in UTC; convert to local time in visualization layers.

---

## 6. Testing Strategies for Event Emission

Before production:

1. **Unit test emitter**

   * Ensure JSON serialization works for all expected field types.
   * Verify timestamps are correctly formatted.

2. **Schema validation**

   * Optional: use JSON Schema to validate events before writing.
   * In LLMOps, schema drift is common; decide if strict validation is worth it.

3. **Volume test**

   * Simulate thousands of events to see if buffering and flushing works.
   * Check for write contention if multiple processes log to same file.

4. **Failure injection**

   * Simulate network or tool call failures and ensure `action_result` logs `status="error"`.

5. **Cross-process test**

   * Run multiple agents writing to separate files; verify ingestion still works.

---

## 7. Insights for Real-world Projects

* Event logs **complement** metrics — metrics tell you *what*, events tell you *why*.
* Even a simple local JSONL logger is enough for basic observability in development.
* You can later stream these events to Kafka, Kinesis, or a monitoring API without changing the emission code — just swap out `JsonlSink`.
* Store events raw — don’t mutate them on ingestion. All enrichment should be done downstream.

---

## 8. Gotchas and Pitfalls

* **Buffer loss**: If your process crashes before `flush()`, you lose buffered events.
* **Mixed data types**: Be consistent in your field types (e.g., `duration_s` always numeric).
* **File growth**: JSONL logs can grow large quickly. Rotate or compress in production.
* **Time skew**: If your system clock is wrong, ordering and duration metrics will be unreliable.

